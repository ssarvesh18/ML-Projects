{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent learns to preform in its environment by performing actions and seeing results\n",
    "# Has  main components i)Agent  ii)Environment\n",
    "# Environment sends a state to the agent the agent performs an action based on observation\n",
    "# The environment then sends the next state and a reward to the agent\n",
    "# The agent will then update its knowledge with the reward and use it to evaluate its previous action\n",
    "# This process continues till the agent sends its terminal task\n",
    "# AGENT: the reinforcement learning algorithmnthat learns from trial and error\n",
    "# ENVIRONMENT: the world through which the agent moves\n",
    "# ACTIONS(A): all the posssibe steps an agent can take\n",
    "# STATE(S): current condition returned by the environment\n",
    "# REWARD(R): an instant return from the environment to appraise the last action\n",
    "# POLICY(pi): the approach the agent uses to determine the next action based on the current state\n",
    "# Value(V): the expected long term return with discout as opposed to short term reward R\n",
    "# ACTION-VALUE(Q): similar to value except it takes an extra parameter,the current action (A)\n",
    "# gamma lies b/w 0 and 1. smaller the gamma larger the discount less exploration/risk\n",
    "# Rl Agent must be trained in such a way the it takes the best action,so as to maximize the reward \n",
    "# Scrificing a bigger reward due to risk is called discounting\n",
    "# EXPLOITATION: using the already known information to heighten the rewards\n",
    "# EXPLORATION: exploring and obtaining more information about the environment\n",
    "# MARKOVS DECISION PROCESS:-\n",
    "# i) set of Action A\n",
    "# ii) set of States S\n",
    "# iii) Reward R\n",
    "# iv) Policy pi\n",
    "# v) Value V\n",
    "# Q-LEARNING ALGORITHM\n",
    "# i) Q Matrix represents the memory of what the agent has learnt through experience\n",
    "#    Rows of Q Matrix represents the current state\n",
    "#    columns represent the possible actions leading to the next state\n",
    "#    Q(state,action)=R(state,action)+Gamma*Max[Q(next state,all actions)]\n",
    "#    if gamma is closer to zero => Agent only looks for immediate rewards (sticks to a certain policy)\n",
    "#    if gamma is closer to one => Agent will consider future rewards with greater weights (explores the entire evironment and \n",
    "#                                                                                          choose an optimum policy)\n",
    "# STEPS:-\n",
    "# i)intitialize gamma value\n",
    "# ii)initialize Q Matrix value to to zero\n",
    "# iii)select random initial state\n",
    "# iv)set initial state to current state\n",
    "# v)select one among all possible actions\n",
    "# vi)using this possible action try going to next state\n",
    "# vii)Get maximum Q value for this next state by considering all possible actions\n",
    "# viii)Compute: Q(state,action)=R(state,action)+Gamma*Max[Q(next state,all actions)]\n",
    "# ix)Repeat steps untill goal is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=np.matrix([[-1,-1,-1,-1,0,-1],[-1,-1,-1,0,-1,100],[-1,-1,-1,0,-1,-1],[-1,0,0,-1,0,-1],[0,-1,-1,0,-1,100],[-1,0,-1,-1,0,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=np.matrix(np.zeros([6,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibilities=np.where(q[initial_state,]>=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-1, -1, -1,  0, -1, -1]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
